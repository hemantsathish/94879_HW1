{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9566d1f1-f9b8-45fd-be6e-535b4f5d0eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install xgboost lightgbm catboost scikit-optimize --quiet\n",
    "\n",
    "# Restart Python kernel to load new packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8f588d-dc99-40eb-a720-510f1490df99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mInvalidParameterError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8103110435742142>, line 478\u001B[0m\n",
       "\u001B[1;32m    475\u001B[0m     importance_title \u001B[38;5;241m=\u001B[39m best_name\n",
       "\u001B[1;32m    477\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCalculating permutation importance for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimportance_title\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 478\u001B[0m pi \u001B[38;5;241m=\u001B[39m permutation_importance(\n",
       "\u001B[1;32m    479\u001B[0m     importance_model, X_test\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m2000\u001B[39m], y_test\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m2000\u001B[39m],\n",
       "\u001B[1;32m    480\u001B[0m     n_repeats\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    481\u001B[0m )\n",
       "\u001B[1;32m    483\u001B[0m imp_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\n",
       "\u001B[1;32m    484\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m: X_test\u001B[38;5;241m.\u001B[39mcolumns,\n",
       "\u001B[1;32m    485\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m: pi\u001B[38;5;241m.\u001B[39mimportances_mean\n",
       "\u001B[1;32m    486\u001B[0m })\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m20\u001B[39m)\n",
       "\u001B[1;32m    488\u001B[0m \u001B[38;5;66;03m# Visualization\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:203\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m to_ignore \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcls\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m    201\u001B[0m params \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m to_ignore}\n",
       "\u001B[0;32m--> 203\u001B[0m validate_parameter_constraints(\n",
       "\u001B[1;32m    204\u001B[0m     parameter_constraints, params, caller_name\u001B[38;5;241m=\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\n",
       "\u001B[1;32m    205\u001B[0m )\n",
       "\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n",
       "\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n",
       "\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n",
       "\u001B[1;32m    211\u001B[0m         )\n",
       "\u001B[1;32m    212\u001B[0m     ):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:95\u001B[0m, in \u001B[0;36mvalidate_parameter_constraints\u001B[0;34m(parameter_constraints, params, caller_name)\u001B[0m\n",
       "\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     90\u001B[0m     constraints_str \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m     91\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;28mstr\u001B[39m(c)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mconstraints[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     92\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     93\u001B[0m     )\n",
       "\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m InvalidParameterError(\n",
       "\u001B[1;32m     96\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_name\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m parameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcaller_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     97\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_val\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     98\u001B[0m )\n",
       "\n",
       "\u001B[0;31mInvalidParameterError\u001B[0m: The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got None instead."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "InvalidParameterError",
        "evalue": "The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got None instead."
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mInvalidParameterError\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-8103110435742142>, line 478\u001B[0m\n\u001B[1;32m    475\u001B[0m     importance_title \u001B[38;5;241m=\u001B[39m best_name\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCalculating permutation importance for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimportance_title\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 478\u001B[0m pi \u001B[38;5;241m=\u001B[39m permutation_importance(\n\u001B[1;32m    479\u001B[0m     importance_model, X_test\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m2000\u001B[39m], y_test\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m2000\u001B[39m],\n\u001B[1;32m    480\u001B[0m     n_repeats\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    481\u001B[0m )\n\u001B[1;32m    483\u001B[0m imp_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\n\u001B[1;32m    484\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m: X_test\u001B[38;5;241m.\u001B[39mcolumns,\n\u001B[1;32m    485\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m: pi\u001B[38;5;241m.\u001B[39mimportances_mean\n\u001B[1;32m    486\u001B[0m })\u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m20\u001B[39m)\n\u001B[1;32m    488\u001B[0m \u001B[38;5;66;03m# Visualization\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:203\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    200\u001B[0m to_ignore \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcls\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    201\u001B[0m params \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m to_ignore}\n\u001B[0;32m--> 203\u001B[0m validate_parameter_constraints(\n\u001B[1;32m    204\u001B[0m     parameter_constraints, params, caller_name\u001B[38;5;241m=\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\n\u001B[1;32m    205\u001B[0m )\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:95\u001B[0m, in \u001B[0;36mvalidate_parameter_constraints\u001B[0;34m(parameter_constraints, params, caller_name)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     90\u001B[0m     constraints_str \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     91\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;28mstr\u001B[39m(c)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mconstraints[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     92\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     93\u001B[0m     )\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m InvalidParameterError(\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_name\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m parameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcaller_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_val\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     98\u001B[0m )\n",
        "\u001B[0;31mInvalidParameterError\u001B[0m: The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got None instead."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import mlflow, mlflow.sklearn, mlflow.xgboost\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, os, warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import randint as sp_randint, uniform\n",
    "from scipy.optimize import minimize\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Config\n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "experiment_name = f\"/Users/{username}/air-quality-prediction\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "DATA_PATH = 'AirQualityUCI.csv'\n",
    "\n",
    "# Load and Clean Data\n",
    "df = pd.read_csv(DATA_PATH, sep=';', decimal=',', na_values=[-200, -200.0])\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H.%M.%S', errors='coerce')\n",
    "\n",
    "df = df.drop(columns=['Date', 'Time']).dropna(subset=['DateTime']).sort_values('DateTime')\n",
    "df = df.set_index('DateTime')\n",
    "\n",
    "for c in df.select_dtypes(np.number).columns:\n",
    "    df[c] = df[c].interpolate(method='time', limit_direction='both')\n",
    "df = df.reset_index().dropna()\n",
    "\n",
    "# Feature Engineering\n",
    "target_col = 'CO(GT)'\n",
    "df = df.sort_values('DateTime').set_index('DateTime')\n",
    "\n",
    "# Creating Lag Features\n",
    "lag_periods = [1, 2, 3, 6, 12, 24, 48, 72]\n",
    "for lag in lag_periods:\n",
    "    df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "\n",
    "# Creating Rolling Features\n",
    "windows = [3, 6, 12, 24, 48, 168]\n",
    "for w in windows:\n",
    "    roll = df[target_col].shift(1).rolling(window=w, min_periods=1)\n",
    "    for stat, func in {'mean': roll.mean(), 'std': roll.std(), 'min': roll.min(), 'max': roll.max()}.items():\n",
    "        df[f'{target_col}_rolling_{stat}_{w}'] = func\n",
    "\n",
    "# Creating Rate of Change Features\n",
    "pollutants = ['PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'NOx(GT)', 'NO2(GT)']\n",
    "\n",
    "for col in pollutants:\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_diff_1h'] = df[col].diff(1)\n",
    "        df[f'{col}_diff_3h'] = df[col].diff(3)\n",
    "        df[f'{col}_diff_24h'] = df[col].diff(24)\n",
    "\n",
    "# Creating Pollutant Interaction Features\n",
    "for i, p1 in enumerate(pollutants):\n",
    "    if p1 in df.columns:\n",
    "        for p2 in pollutants[i+1:]:\n",
    "            if p2 in df.columns:\n",
    "                df[f'{p1}_x_{p2}'] = df[p1] * df[p2]\n",
    "                df[f'{p1}_ratio_{p2}'] = df[p1] / (df[p2] + 1e-8)\n",
    "\n",
    "# 5) Creating Environmental Interaction Features\n",
    "if 'T' in df.columns and 'AH' in df.columns:\n",
    "    df['temp_humidity'] = df['T'] * df['AH']\n",
    "    df['temp_sq'] = df['T'] ** 2\n",
    "    df['humidity_sq'] = df['AH'] ** 2\n",
    "\n",
    "if 'T' in df.columns and 'RH' in df.columns:\n",
    "    df['temp_rh'] = df['T'] * df['RH']\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "# Creating Time Based Features\n",
    "df['hour'] = df['DateTime'].dt.hour\n",
    "df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "df['month'] = df['DateTime'].dt.month\n",
    "df['day_of_month'] = df['DateTime'].dt.day\n",
    "df['week_of_year'] = df['DateTime'].dt.isocalendar().week\n",
    "\n",
    "# Creating cyclical encodings\n",
    "df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "df['dow_sin'] = np.sin(2*np.pi*df['day_of_week']/7)\n",
    "df['dow_cos'] = np.cos(2*np.pi*df['day_of_week']/7)\n",
    "df['month_sin'] = np.sin(2*np.pi*(df['month']-1)/12)\n",
    "df['month_cos'] = np.cos(2*np.pi*(df['month']-1)/12)\n",
    "\n",
    "# Creating categorical time features\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "df['is_night'] = df['hour'].isin([22, 23, 0, 1, 2, 3, 4, 5]).astype(int)\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "\n",
    "# Drop rows with NaN created by lagging/diffing\n",
    "df = df.dropna()\n",
    "\n",
    "# Build out features\n",
    "exclude = ['DateTime', target_col]\n",
    "X = df.drop(columns=exclude)\n",
    "y = df[target_col]\n",
    "X = X.select_dtypes(include=[np.number]).copy()\n",
    "feature_cols = X.columns.tolist()\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Dataset size: {len(X)}\")\n",
    "\n",
    "# Data Split\n",
    "n = len(X)\n",
    "t1, t2 = int(0.7*n), int(0.85*n)\n",
    "X_train, y_train = X.iloc[:t1], y.iloc[:t1]\n",
    "X_valid, y_valid = X.iloc[t1:t2], y.iloc[t1:t2]\n",
    "X_test, y_test = X.iloc[t2:], y.iloc[t2:]\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Valid: {len(X_valid)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Mean Absolute Percentage Error\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred)\n",
    "\n",
    "# Symmetric Mean Absolute Percentage Error\n",
    "def smape(y_true, y_pred, eps=1e-8):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2\n",
    "    return np.mean(np.abs(y_pred - y_true) / denom) * 100\n",
    "\n",
    "# Mean Absolute Scaled Error\n",
    "def mase(y_true, y_pred, y_hist):\n",
    "    denom = np.mean(np.abs(np.diff(y_hist)))\n",
    "    return np.mean(np.abs(y_true - y_pred)) / (denom + 1e-8)\n",
    "\n",
    "# Metrics\n",
    "def metrics(y_true, y_pred, y_hist):\n",
    "    return dict(\n",
    "        rmse=np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        mae=mean_absolute_error(y_true, y_pred),\n",
    "        r2=r2_score(y_true, y_pred),\n",
    "        mape=np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100,\n",
    "        smape=smape(y_true, y_pred),\n",
    "        mase=mase(y_true, y_pred, y_hist)\n",
    "    )\n",
    "\n",
    "def make_ttr(estimator):\n",
    "    return TransformedTargetRegressor(\n",
    "        estimator, func=np.log1p, inverse_func=np.expm1\n",
    "    ) if y_train.min() >= 0 else estimator\n",
    "\n",
    "# Bayesian Optimization Search\n",
    "tscv = TimeSeriesSplit(n_splits=7)\n",
    "\n",
    "def run_bayes_search(name, base_est, search_space, n_iter=50):\n",
    "    # Bayesian optimization for hyperparameter tuning\n",
    "    with mlflow.start_run(run_name=f\"bayes_{name}\"):\n",
    "        search = BayesSearchCV(\n",
    "            estimator=make_ttr(base_est),\n",
    "            search_spaces=search_space,\n",
    "            n_iter=n_iter,\n",
    "            cv=tscv,\n",
    "            scoring=\"neg_root_mean_squared_error\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best = search.best_estimator_\n",
    "        \n",
    "        # Log parameters\n",
    "        for k, v in search.best_params_.items():\n",
    "            mlflow.log_param(k, v)\n",
    "        \n",
    "        # Validate\n",
    "        yv = best.predict(X_valid)\n",
    "        for k, v in metrics(y_valid, yv, y_train).items():\n",
    "            mlflow.log_metric(f\"valid_{k}\", v)\n",
    "        \n",
    "        mlflow.sklearn.log_model(best, \"model\")\n",
    "        print(f\"✓ {name:20s} valid RMSE={metrics(y_valid, yv, y_train)['rmse']:.4f}\")\n",
    "        return best\n",
    "\n",
    "# Model Training with Bayesian Optimization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODELS WITH BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# RandomForest\n",
    "rf_space = {\n",
    "    \"regressor__n_estimators\": Integer(300, 1000),\n",
    "    \"regressor__max_depth\": Integer(8, 40),\n",
    "    \"regressor__min_samples_split\": Integer(2, 20),\n",
    "    \"regressor__min_samples_leaf\": Integer(1, 10),\n",
    "    \"regressor__max_features\": Real(0.3, 1.0),\n",
    "}\n",
    "best_rf = run_bayes_search(\"RandomForest\", \n",
    "    RandomForestRegressor(n_jobs=-1, random_state=42), rf_space, n_iter=40)\n",
    "\n",
    "# ExtraTrees\n",
    "et_space = {\n",
    "    \"regressor__n_estimators\": Integer(400, 1200),\n",
    "    \"regressor__max_depth\": Integer(8, 40),\n",
    "    \"regressor__min_samples_split\": Integer(2, 20),\n",
    "    \"regressor__min_samples_leaf\": Integer(1, 10),\n",
    "}\n",
    "best_et = run_bayes_search(\"ExtraTrees\",\n",
    "    ExtraTreesRegressor(n_jobs=-1, random_state=42), et_space, n_iter=40)\n",
    "\n",
    "# GradientBoosting\n",
    "gb_space = {\n",
    "    \"regressor__n_estimators\": Integer(300, 1500),\n",
    "    \"regressor__learning_rate\": Real(0.01, 0.2, prior='log-uniform'),\n",
    "    \"regressor__max_depth\": Integer(3, 8),\n",
    "    \"regressor__min_samples_split\": Integer(2, 20),\n",
    "    \"regressor__subsample\": Real(0.6, 1.0),\n",
    "}\n",
    "best_gb = run_bayes_search(\"GradientBoosting\",\n",
    "    GradientBoostingRegressor(random_state=42), gb_space, n_iter=40)\n",
    "\n",
    "# LightGBM\n",
    "lgb_space = {\n",
    "    \"regressor__n_estimators\": Integer(500, 2000),\n",
    "    \"regressor__max_depth\": Integer(4, 15),\n",
    "    \"regressor__learning_rate\": Real(0.01, 0.2, prior='log-uniform'),\n",
    "    \"regressor__num_leaves\": Integer(20, 200),\n",
    "    \"regressor__min_child_samples\": Integer(10, 100),\n",
    "    \"regressor__subsample\": Real(0.6, 1.0),\n",
    "    \"regressor__colsample_bytree\": Real(0.6, 1.0),\n",
    "    \"regressor__reg_alpha\": Real(0, 0.5),\n",
    "    \"regressor__reg_lambda\": Real(0, 0.5),\n",
    "}\n",
    "best_lgb = run_bayes_search(\"LightGBM\",\n",
    "    lgb.LGBMRegressor(n_jobs=-1, random_state=42, verbosity=-1), lgb_space, n_iter=50)\n",
    "\n",
    "# CatBoost\n",
    "cat_space = {\n",
    "    \"regressor__iterations\": Integer(500, 2000),\n",
    "    \"regressor__depth\": Integer(4, 10),\n",
    "    \"regressor__learning_rate\": Real(0.01, 0.2, prior='log-uniform'),\n",
    "    \"regressor__l2_leaf_reg\": Real(1, 10),\n",
    "    \"regressor__border_count\": Integer(32, 255),\n",
    "}\n",
    "best_cat = run_bayes_search(\"CatBoost\",\n",
    "    CatBoostRegressor(random_state=42, verbose=0), cat_space, n_iter=50)\n",
    "\n",
    "# XGBoost with early stopping\n",
    "print(\"\\nTraining XGBoost with early stopping...\")\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "with mlflow.start_run(run_name=\"bayes_XGBoost\"):\n",
    "    # Bayesian search for hyperparameters\n",
    "    xgb_space = {\n",
    "        \"n_estimators\": Integer(500, 2500),\n",
    "        \"max_depth\": Integer(3, 12),\n",
    "        \"learning_rate\": Real(0.01, 0.3, prior='log-uniform'),\n",
    "        \"subsample\": Real(0.6, 1.0),\n",
    "        \"colsample_bytree\": Real(0.5, 1.0),\n",
    "        \"min_child_weight\": Integer(1, 20),\n",
    "        \"gamma\": Real(0, 0.5),\n",
    "        \"reg_alpha\": Real(0, 1.0),\n",
    "        \"reg_lambda\": Real(0, 2.0),\n",
    "    }\n",
    "    \n",
    "    base_xgb = xgb.XGBRegressor(tree_method=\"hist\", objective=\"reg:squarederror\", random_state=42)\n",
    "    search = BayesSearchCV(\n",
    "        base_xgb, xgb_space, n_iter=50, cv=tscv,\n",
    "        scoring=\"neg_root_mean_squared_error\", n_jobs=-1, random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    best_p = search.best_params_\n",
    "    \n",
    "    for k, v in best_p.items():\n",
    "        mlflow.log_param(k, v)\n",
    "    \n",
    "    # Early stopping with DMatrix\n",
    "    use_log = float(y_train.min()) >= 0.0\n",
    "    y_tr = np.log1p(y_train).values if use_log else y_train.values\n",
    "    y_va = np.log1p(y_valid).values if use_log else y_valid.values\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_tr)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_va)\n",
    "    \n",
    "    train_params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": int(best_p[\"max_depth\"]),\n",
    "        \"eta\": float(best_p[\"learning_rate\"]),\n",
    "        \"subsample\": float(best_p[\"subsample\"]),\n",
    "        \"colsample_bytree\": float(best_p[\"colsample_bytree\"]),\n",
    "        \"min_child_weight\": float(best_p[\"min_child_weight\"]),\n",
    "        \"gamma\": float(best_p.get(\"gamma\", 0.0)),\n",
    "        \"alpha\": float(best_p.get(\"reg_alpha\", 0.0)),\n",
    "        \"lambda\": float(best_p.get(\"reg_lambda\", 1.0)),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    \n",
    "    booster = xgb.train(\n",
    "        params=train_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=int(best_p.get(\"n_estimators\", 1500)),\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Get best iteration\n",
    "    best_n = None\n",
    "    for attr in (\"best_ntree_limit\", \"best_iteration\"):\n",
    "        if hasattr(booster, attr) and getattr(booster, attr) is not None:\n",
    "            val = getattr(booster, attr)\n",
    "            best_n = int(val if \"ntree\" in attr else (val + 1))\n",
    "            break\n",
    "    if best_n is None:\n",
    "        best_n = int(best_p.get(\"n_estimators\", 1000))\n",
    "    \n",
    "    # Final model on train+valid\n",
    "    X_all = pd.concat([X_train, X_valid], axis=0)\n",
    "    y_all = pd.concat([y_train, y_valid], axis=0)\n",
    "    \n",
    "    final_xgb = xgb.XGBRegressor(\n",
    "        tree_method=\"hist\", objective=\"reg:squarederror\", random_state=42,\n",
    "        **{**best_p, \"n_estimators\": best_n}\n",
    "    )\n",
    "    best_xgb = (TransformedTargetRegressor(final_xgb, func=np.log1p, inverse_func=np.expm1)\n",
    "                if use_log else final_xgb)\n",
    "    best_xgb.fit(X_all, y_all)\n",
    "    \n",
    "    yv = best_xgb.predict(X_valid)\n",
    "    val_m = metrics(y_valid, yv, y_train)\n",
    "    for k, v in val_m.items():\n",
    "        mlflow.log_metric(f\"valid_{k}\", float(v))\n",
    "    \n",
    "    signature = infer_signature(X_test, best_xgb.predict(X_test))\n",
    "    mlflow.sklearn.log_model(best_xgb, \"model\", signature=signature)\n",
    "    print(f\"✓ {'XGBoost':20s} valid RMSE={val_m['rmse']:.4f}\")\n",
    "\n",
    "# Weighted Ensemble\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING OPTIMIZED WEIGHTED ENSEMBLE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Get validation predictions from all models\n",
    "models_dict = {\n",
    "    'RF': best_rf,\n",
    "    'ET': best_et,\n",
    "    'GB': best_gb,\n",
    "    'XGB': best_xgb,\n",
    "    'LGB': best_lgb,\n",
    "    'CAT': best_cat\n",
    "}\n",
    "\n",
    "val_preds = [model.predict(X_valid) for model in models_dict.values()]\n",
    "\n",
    "def weighted_predictions(weights, predictions):\n",
    "    return sum(w * p for w, p in zip(weights, predictions))\n",
    "\n",
    "def objective(weights, predictions, y_true):\n",
    "    weighted = weighted_predictions(weights, predictions)\n",
    "    return mean_squared_error(y_true, weighted)\n",
    "\n",
    "# Optimize weights\n",
    "init_weights = np.ones(len(val_preds)) / len(val_preds)\n",
    "bounds = [(0, 1)] * len(val_preds)\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "\n",
    "result = minimize(\n",
    "    objective, init_weights,\n",
    "    args=(val_preds, y_valid),\n",
    "    bounds=bounds,\n",
    "    constraints=constraints,\n",
    "    method='SLSQP'\n",
    ")\n",
    "\n",
    "optimal_weights = result.x\n",
    "print(\"\\nOptimal Ensemble Weights:\")\n",
    "for name, weight in zip(models_dict.keys(), optimal_weights):\n",
    "    print(f\"  {name:10s}: {weight:.4f}\")\n",
    "\n",
    "# Stacking Ensemble\n",
    "print(\"\\nTraining Stacking Ensemble...\")\n",
    "stack = make_ttr(StackingRegressor(\n",
    "    estimators=list(models_dict.items()),\n",
    "    final_estimator=Ridge(alpha=1.0, random_state=42),\n",
    "    n_jobs=-1\n",
    "))\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Final Evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "def evaluate(name, model, is_weighted=False, weights=None):\n",
    "    with mlflow.start_run(run_name=f\"final_{name}\"):\n",
    "        if is_weighted:\n",
    "            # Weighted ensemble prediction\n",
    "            test_preds = [m.predict(X_test) for m in models_dict.values()]\n",
    "            ypred = weighted_predictions(weights, test_preds)\n",
    "        else:\n",
    "            ypred = model.predict(X_test)\n",
    "        \n",
    "        m = metrics(y_test, ypred, y_train)\n",
    "        for k, v in m.items():\n",
    "            mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "        \n",
    "        if not is_weighted:\n",
    "            signature = infer_signature(X_test, ypred)\n",
    "            mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "        \n",
    "        print(f\"{name:20s}: RMSE={m['rmse']:.4f}, MAE={m['mae']:.4f}, R²={m['r2']:.4f}, SMAPE={m['smape']:.3f}%\")\n",
    "        return m\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "results['RandomForest'] = evaluate(\"RandomForest\", best_rf)\n",
    "results['ExtraTrees'] = evaluate(\"ExtraTrees\", best_et)\n",
    "results['GradientBoosting'] = evaluate(\"GradientBoosting\", best_gb)\n",
    "results['XGBoost'] = evaluate(\"XGBoost\", best_xgb)\n",
    "results['LightGBM'] = evaluate(\"LightGBM\", best_lgb)\n",
    "results['CatBoost'] = evaluate(\"CatBoost\", best_cat)\n",
    "results['StackEnsemble'] = evaluate(\"StackEnsemble\", stack)\n",
    "results['WeightedEnsemble'] = evaluate(\"WeightedEnsemble\", None, \n",
    "                                       is_weighted=True, weights=optimal_weights)\n",
    "\n",
    "# Model Comparision\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "comp_df = pd.DataFrame(results).T.sort_values('rmse')\n",
    "display(comp_df)\n",
    "\n",
    "# Find best model\n",
    "best_name = comp_df.index[0]\n",
    "print(f\"\\nBEST MODEL: {best_name}\")\n",
    "print(f\"   RMSE: {comp_df.loc[best_name, 'rmse']:.4f}\")\n",
    "print(f\"   R²:   {comp_df.loc[best_name, 'r2']:.4f}\")\n",
    "print(f\"   SMAPE: {comp_df.loc[best_name, 'smape']:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de461e5-03dd-4ff0-a5d3-a9c2a4152e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'RandomForest':     best_rf,\n",
    "    'ExtraTrees':       best_et,\n",
    "    'GradientBoosting': best_gb,\n",
    "    'XGBoost':          best_xgb,\n",
    "    'LightGBM':         best_lgb,\n",
    "    'CatBoost':         best_cat,\n",
    "}\n",
    "\n",
    "# Permutation Importance\n",
    "def resolve_model(name: str):\n",
    "    if name == 'StackEnsemble':\n",
    "        return stack\n",
    "    if name == 'WeightedEnsemble':\n",
    "        # Pick best individual (excluding WeightedEnsemble)\n",
    "        best_individual = comp_df.drop('WeightedEnsemble').index[0]\n",
    "        return resolve_model(best_individual)\n",
    "    # otherwise a plain model from models_dict\n",
    "    return models_dict[name]\n",
    "\n",
    "importance_model = resolve_model(best_name)\n",
    "importance_title = best_name if best_name != 'WeightedEnsemble' else f\"{comp_df.drop('WeightedEnsemble').index[0]} (from Weighted)\"\n",
    "\n",
    "# Safety check\n",
    "if importance_model is None:\n",
    "    raise ValueError(f\"Could not resolve model for importance: {best_name}\")\n",
    "\n",
    "print(f\"\\nCalculating permutation importance for {importance_title}...\")\n",
    "pi = permutation_importance(\n",
    "    importance_model,\n",
    "    X_test.iloc[:2000], y_test.iloc[:2000],\n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "imp_df = (pd.DataFrame({'feature': X_test.columns, 'importance': pi.importances_mean})\n",
    "            .sort_values('importance', ascending=False)\n",
    "            .head(20))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top features\n",
    "axes[0].barh(imp_df['feature'].iloc[::-1], imp_df['importance'].iloc[::-1])\n",
    "axes[0].set_title(f'Top 20 Features — {importance_title}')\n",
    "axes[0].set_xlabel('Permutation Importance')\n",
    "\n",
    "# Predictions vs Actual\n",
    "if best_name == 'WeightedEnsemble':\n",
    "    test_preds = [m.predict(X_test) for m in models_dict.values()]\n",
    "    test_pred = np.sum(optimal_weights * np.vstack(test_preds), axis=0)\n",
    "else:\n",
    "    test_pred = importance_model.predict(X_test)\n",
    "\n",
    "axes[1].scatter(y_test, test_pred, alpha=0.5, s=20)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual CO(GT)')\n",
    "axes[1].set_ylabel('Predicted CO(GT)')\n",
    "axes[1].set_title(f'Predictions vs Actual — {best_name}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea452d0-463a-4072-8266-0b38c98ee230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save XGBoost model\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING XGBOOST MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run ID\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"tags.mlflow.runName = 'bayes_XGBoost'\",\n",
    "    order_by=[\"start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "if not runs:\n",
    "    print(\"ERROR: XGBoost run not found\")\n",
    "else:\n",
    "    run_id = runs[0].info.run_id\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    \n",
    "    print(f\"\\nRun ID: {run_id}\")\n",
    "    print(f\"Model URI: {model_uri}\")\n",
    "    \n",
    "    # Get XGBoost metrics\n",
    "    xgb_metrics = results.get('XGBoost', {})\n",
    "    \n",
    "    # Register with MLFlow Model Registry\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"REGISTERING TO MODEL REGISTRY\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    registered_model_name = \"xgboost_final_best\"\n",
    "    \n",
    "    try:\n",
    "        model_version = mlflow.register_model(model_uri, registered_model_name)\n",
    "        \n",
    "        print(f\"Model registered: {registered_model_name}\")\n",
    "        print(f\"Version: {model_version.version}\")\n",
    "        \n",
    "        description = f\"\"\"XGBoost model for CO(GT) air quality prediction\n",
    "\n",
    "            Performance Metrics (Test Set):\n",
    "            - RMSE: {xgb_metrics.get('rmse', 0):.4f}\n",
    "            - MAE: {xgb_metrics.get('mae', 0):.4f}\n",
    "            - R²: {xgb_metrics.get('r2', 0):.4f}\n",
    "            - MAPE: {xgb_metrics.get('mape', 0):.2f}%\n",
    "            - SMAPE: {xgb_metrics.get('smape', 0):.2f}%\n",
    "\n",
    "            Model Details:\n",
    "            - Algorithm: XGBoost with Bayesian optimization and early stopping\n",
    "            - Features: {len(feature_cols)} engineered features\n",
    "            - Hyperparameter Search: BayesSearchCV (50 iterations)\n",
    "            - Target Transform: Log transformation\n",
    "            - Dataset: UCI Air Quality Dataset\n",
    "\n",
    "            Enhanced Features:\n",
    "            - Extended lag features (1-72 hours)\n",
    "            - Rolling statistics (3h to 1 week)\n",
    "            - Rate of change features\n",
    "            - Pollutant interaction features\n",
    "            - Environmental interactions\n",
    "            - Time-based categorical features\n",
    "            \"\"\"\n",
    "        \n",
    "        client.update_model_version(\n",
    "            name=registered_model_name,\n",
    "            version=model_version.version,\n",
    "            description=description\n",
    "        )\n",
    "        \n",
    "        print(\"Description updated\")\n",
    "        \n",
    "        # Try to transition to Production\n",
    "        try:\n",
    "            client.transition_model_version_stage(\n",
    "                name=registered_model_name,\n",
    "                version=model_version.version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            print(\"Stage: Production\")\n",
    "        except Exception as stage_error:\n",
    "            print(f\"Stage transition skipped: {str(stage_error)[:50]}\")\n",
    "            print(\"Set stage manually in MLflow UI if needed\")\n",
    "        \n",
    "        # Add tags\n",
    "        try:\n",
    "            client.set_model_version_tag(\n",
    "                name=registered_model_name,\n",
    "                version=model_version.version,\n",
    "                key=\"model_type\",\n",
    "                value=\"XGBoost\"\n",
    "            )\n",
    "            client.set_model_version_tag(\n",
    "                name=registered_model_name,\n",
    "                version=model_version.version,\n",
    "                key=\"optimization\",\n",
    "                value=\"Bayesian\"\n",
    "            )\n",
    "            print(\"Tags added\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except Exception as reg_error:\n",
    "        print(f\"Registration error: {str(reg_error)[:100]}\")\n",
    "    \n",
    "    # Save model offline\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"EXPORTING MODEL OFFLINE\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    \n",
    "    # Download MLflow artifacts\n",
    "    local_dir = \"./data/xgboost_model\"\n",
    "    if os.path.exists(local_dir):\n",
    "        shutil.rmtree(local_dir)\n",
    "    \n",
    "    # Create directory for download\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    artifact_path = client.download_artifacts(run_id, \"model\", local_dir)\n",
    "    print(f\"Model downloaded to: {artifact_path}\")\n",
    "    \n",
    "    # Create zip\n",
    "    zip_base = \"./data/xgboost_model\"\n",
    "    shutil.make_archive(zip_base, 'zip', local_dir)\n",
    "    print(f\"Zipped: {zip_base}.zip\")\n",
    "    \n",
    "    # Save as pickle\n",
    "    pickle_path = \"./data/xgboost_model.pkl\"\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(best_xgb, f)\n",
    "    print(f\"Pickle: {pickle_path}\")\n",
    "    \n",
    "    # Create features.json\n",
    "    features_spec = {\n",
    "        \"model_name\": \"XGBoost Air Quality CO Prediction (Enhanced)\",\n",
    "        \"target\": target_col,\n",
    "        \"n_features\": len(feature_cols),\n",
    "        \"feature_names\": feature_cols,\n",
    "        \"feature_categories\": {\n",
    "            \"lag_features\": [f for f in feature_cols if 'lag' in f],\n",
    "            \"rolling_features\": [f for f in feature_cols if 'rolling' in f],\n",
    "            \"rate_of_change\": [f for f in feature_cols if 'diff' in f],\n",
    "            \"interactions\": [f for f in feature_cols if any(x in f for x in ['_x_', '_ratio_'])],\n",
    "            \"temporal\": [f for f in feature_cols if any(x in f for x in ['hour', 'day', 'month', 'sin', 'cos', 'weekend', 'rush', 'night', 'winter', 'summer'])]\n",
    "        },\n",
    "        \"preprocessing\": {\n",
    "            \"target_transform\": \"log1p\",\n",
    "            \"feature_scaling\": \"none\",\n",
    "            \"missing_values\": \"forward/backward interpolation\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    features_path = \"./data/features.json\"\n",
    "    with open(features_path, 'w') as f:\n",
    "        json.dump(features_spec, f, indent=2)\n",
    "    print(f\"Features spec: {features_path}\")\n",
    "    \n",
    "    # Create model metadata\n",
    "    metadata = {\n",
    "        \"model_info\": {\n",
    "            \"name\": registered_model_name if 'model_version' in locals() else \"xgboost_final_best\",\n",
    "            \"version\": model_version.version if 'model_version' in locals() else \"N/A\",\n",
    "            \"run_id\": run_id,\n",
    "            \"algorithm\": \"XGBoost with Bayesian Optimization\"\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"test_rmse\": float(xgb_metrics.get('rmse', 0)),\n",
    "            \"test_mae\": float(xgb_metrics.get('mae', 0)),\n",
    "            \"test_r2\": float(xgb_metrics.get('r2', 0)),\n",
    "            \"test_mape\": float(xgb_metrics.get('mape', 0)),\n",
    "            \"test_smape\": float(xgb_metrics.get('smape', 0)),\n",
    "            \"test_mase\": float(xgb_metrics.get('mase', 0))\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"train_samples\": len(X_train),\n",
    "            \"valid_samples\": len(X_valid),\n",
    "            \"test_samples\": len(X_test),\n",
    "            \"total_features\": len(feature_cols)\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"load_mlflow\": 'mlflow.pyfunc.load_model(\"./data/xgboost_model/model\")',\n",
    "            \"load_pickle\": \"pickle.load(open('./data/xgboost_model.pkl', 'rb'))\",\n",
    "            \"predict\": \"predictions = model.predict(X)\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = \"./data/model_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Metadata: {metadata_path}\")\n",
    "    \n",
    "    # Example input for testing\n",
    "    example_input = X_test.iloc[0].to_dict()\n",
    "    example_path = \"./data/example_input.json\"\n",
    "    with open(example_path, 'w') as f:\n",
    "        json.dump(example_input, f, indent=2)\n",
    "    print(f\"Example input: {example_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b048a7dd-01ca-4a89-976e-6494581039ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimized Training Final Submission",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}